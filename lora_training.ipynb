{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import json\n",
    "import locale\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "from huggingface_hub import interpreter_login, snapshot_download, whoami, upload_folder, create_repo\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "from lora_utils import create_image_grid\n",
    "from train_dreambooth_lora_sdxl import save_model_card"
   ],
   "id": "a374a33799572ec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_DIR = \"./tio/\"\n",
    "HUGGINGFACE_DATASET = \"mtailanian/tio\"\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "765978ec541c3992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download data",
   "id": "7984c3145f26bd51"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "snapshot_download(\n",
    "    HUGGINGFACE_DATASET,\n",
    "    local_dir=DATA_DIR,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show the data",
   "id": "a8f8ffe5992204a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_extensions = [\".png\", \".jpg\", \".jpeg\", \"JPG\"]\n",
    "\n",
    "images_paths = [str(path) for ext in image_extensions for path in Path(DATA_DIR).glob(f\"*{ext}\")]\n",
    "images = [Image.open(path) for path in images_paths]\n",
    "\n",
    "num_images_to_preview = 5\n",
    "create_image_grid(images[:num_images_to_preview], 1, num_images_to_preview)"
   ],
   "id": "5f4daf28df84953e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Add captions for each image (automatically or not)\n",
    "\n",
    "Al T칤o le vamos a asignar un Token especial.\n",
    "Idealmente ser칤a uno que se use muy poco en el modelo actual.\n",
    "Adem치s es mejor que sea corto. Las palabras largas se dividen en pedazos m치s chicos al tokenizarse, y cada pedacito puede tener alg칰n significado asociado.\n",
    "\n",
    "Y queremos agregar un concepto nuevo. El concepto T칤o. Sin prejuicios.\n",
    "\n",
    "Vamos a crear un archivo con una descripci칩n de cada imagen. La descripci칩n tendr치 un prefijo que lleve este token, y una descripci칩n autogenerada con un modelo de descripci칩n de im치genes: BLIP"
   ],
   "id": "558a6aabf92c2093"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Crear captions autom치ticos",
   "id": "fc907ec804e0a2a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "def caption_image(input_image):\n",
    "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_caption"
   ],
   "id": "4328150ddf1fa930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Crear archivo de captions, incluyendo el Token elegido para representar al T칤o",
   "id": "1b3f2cf9eb45f64a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TIO_TOKEN = \"TOK\"\n",
    "\n",
    "caption_prefix = f\"A photo of {TIO_TOKEN}. \"\n",
    "with open(f\"{Path(DATA_DIR) / 'metadata.jsonl'}\", \"w\") as outfile:\n",
    "  for path, img in zip(images_paths, images):\n",
    "      caption = caption_prefix + caption_image(img).split(\"\\n\")[0]\n",
    "      entry = {\"file_name\":path.split(\"/\")[-1], \"prompt\": caption}\n",
    "      json.dump(entry, outfile)\n",
    "      outfile.write('\\n')"
   ],
   "id": "30c0917f4100ce17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check images and descriptions",
   "id": "2af532e55539e67a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"{Path(DATA_DIR) / 'metadata.jsonl'}\", \"r\") as f:\n",
    "    data = list(map(json.loads, f.read().splitlines()))\n",
    "\n",
    "for d in data[:5]:\n",
    "    img = Image.open(Path(DATA_DIR) / d[\"file_name\"]).resize((256, 256))\n",
    "    print(d[\"file_name\"], d[\"prompt\"])\n",
    "    display(img)"
   ],
   "id": "7a9b3a93ebee8b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# C",
   "id": "13a4e0ae87f59baa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "del blip_processor, blip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "f5ec9a7d97b99c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "accelerate config default"
   ],
   "id": "a35ba423f16d13df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "accelerate launch train_dreambooth_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
    "  --dataset_name=\"tio\" \\\n",
    "  --output_dir=\"output\" \\\n",
    "  --caption_column=\"prompt\"\\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"a photo of TOK\" \\\n",
    "  --resolution=256 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=3 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --snr_gamma=5.0 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --max_train_steps=500 \\\n",
    "  --checkpointing_steps=717 \\\n",
    "  --seed=\"0\""
   ],
   "id": "9ee6db245fb5dca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Login to HuggingFace to save the model",
   "id": "329539e930ff610b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "interpreter_login()",
   "id": "5a609623efde3b65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = \"output\"\n",
    "username = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\n",
    "repo_id = f\"{username}/{model_name}\""
   ],
   "id": "fcd3c95058c00dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push to the hub 游댠",
   "id": "126403c008795a3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "repo_id = create_repo(repo_id, exist_ok=True).repo_id\n",
    "\n",
    "# change the params below according to your training arguments\n",
    "save_model_card(\n",
    "    repo_id = repo_id,\n",
    "    images=[],\n",
    "    base_model=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    train_text_encoder=False,\n",
    "    instance_prompt=\"a photo of TOK\",\n",
    "    validation_prompt=None,\n",
    "    repo_folder=model_name,\n",
    "    vae_path=\"madebyollin/sdxl-vae-fp16-fix\",\n",
    "    use_dora=False,\n",
    ")\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=model_name,\n",
    "    commit_message=\"End of training\",\n",
    "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    ")"
   ],
   "id": "f99e039eae12b750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "32849043f686fd90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "pipe.load_lora_weights(repo_id)\n",
    "_ = pipe.to(\"cuda\")"
   ],
   "id": "50d00885d48be283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = \"a photo of TOK with a hat in Paris with the Eiffel tower\"\n",
    "\n",
    "image = pipe(prompt=prompt, num_inference_steps=25).images[0]\n",
    "\n",
    "display(image)"
   ],
   "id": "57ae0d555ec462ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "55982499d8fda7ba",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
